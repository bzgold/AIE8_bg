{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Setup and Testing\n",
    "\n",
    "This notebook will help you set up and test Ollama with LangChain connectors before starting the main RAG assignment.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Install Ollama** from https://ollama.ai\n",
    "   - On Linux/Mac: `curl https://ollama.ai/install.sh | sh`\n",
    "   - On Windows: Download and run the installer\n",
    "\n",
    "2. **Verify Installation**\n",
    "   - Run `ollama -v` in your terminal\n",
    "   - Should show version 0.11.10 or greater\n",
    "\n",
    "3. **Pull Required Models**\n",
    "   ```bash\n",
    "   # For chat/inference\n",
    "   ollama pull gpt-oss:20b\n",
    "   \n",
    "   # For embeddings\n",
    "   ollama pull embeddinggemma:latest\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Test Ollama Connection\n",
    "\n",
    "First, let's verify that Ollama is running and accessible:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ollama is running!\n",
      "\n",
      "Available models:\n",
      "  - embeddinggemma:latest\n",
      "  - gpt-oss:20b\n",
      "  - mistral:latest\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Test if Ollama is running\n",
    "try:\n",
    "    response = requests.get('http://localhost:11434/api/tags')\n",
    "    if response.status_code == 200:\n",
    "        models = json.loads(response.text)\n",
    "        print(\"✅ Ollama is running!\")\n",
    "        print(\"\\nAvailable models:\")\n",
    "        for model in models.get('models', []):\n",
    "            print(f\"  - {model['name']}\")\n",
    "    else:\n",
    "        print(\"❌ Ollama is not responding properly\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"❌ Cannot connect to Ollama. Make sure it's running!\")\n",
    "    print(\"Start Ollama by running 'ollama serve' in a terminal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test Embeddings with Ollama\n",
    "\n",
    "Now let's test creating embeddings using the LangChain Ollama connector:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding model initialized\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = OllamaEmbeddings(\n",
    "    model=\"embeddinggemma:latest\",\n",
    "    base_url=\"http://localhost:11434\"  # Default Ollama URL\n",
    ")\n",
    "\n",
    "print(\"✅ Embedding model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding query: 'What is the meaning of life?'\n",
      "\n",
      "✅ Successfully created embedding!\n",
      "Embedding dimension: 768\n",
      "First 10 values: [-0.14624307, 0.029132523, 0.037615955, -0.02487349, -0.02655731, 0.016060056, -0.027486855, 0.027256342, 0.01139732, -2.40085e-05]\n"
     ]
    }
   ],
   "source": [
    "# Test embedding a single query\n",
    "test_query = \"What is the meaning of life?\"\n",
    "\n",
    "print(f\"Embedding query: '{test_query}'\")\n",
    "embedding = embedding_model.embed_query(test_query)\n",
    "\n",
    "print(f\"\\n✅ Successfully created embedding!\")\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding multiple documents...\n",
      "\n",
      "✅ Successfully created 3 embeddings!\n",
      "\n",
      "Document 1: 'The quick brown fox jumps over the lazy dog....'\n",
      "  Embedding dimension: 768\n",
      "  First 5 values: [-0.14781645, 0.002890088, 0.052144155, -0.029089162, -0.036695607]\n",
      "\n",
      "Document 2: 'Machine learning is a subset of artificial intelli...'\n",
      "  Embedding dimension: 768\n",
      "  First 5 values: [-0.1240763, -0.0027511106, -0.00032809668, 0.010076388, 0.001677464]\n",
      "\n",
      "Document 3: 'Python is a popular programming language for data ...'\n",
      "  Embedding dimension: 768\n",
      "  First 5 values: [-0.16269195, -0.010295422, 0.025464684, 0.000692403, -0.01887165]\n"
     ]
    }
   ],
   "source": [
    "# Test embedding multiple documents\n",
    "test_documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Python is a popular programming language for data science.\"\n",
    "]\n",
    "\n",
    "print(\"Embedding multiple documents...\")\n",
    "embeddings = embedding_model.embed_documents(test_documents)\n",
    "\n",
    "print(f\"\\n✅ Successfully created {len(embeddings)} embeddings!\")\n",
    "for i, doc in enumerate(test_documents):\n",
    "    print(f\"\\nDocument {i+1}: '{doc[:50]}...'\")\n",
    "    print(f\"  Embedding dimension: {len(embeddings[i])}\")\n",
    "    print(f\"  First 5 values: {embeddings[i][:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Model Inference with Ollama\n",
    "\n",
    "Now let's test using Ollama for text generation/inference using the LangChain connector:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chat model initialized\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize the chat model\n",
    "chat_model = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    temperature=0.6,\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"✅ Chat model initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Let's add a procedure to measure the inference performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_performance_metrics(response_metadata):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive performance metrics from Ollama response metadata\n",
    "    \"\"\"\n",
    "    # Extract all timing data (in nanoseconds)\n",
    "    total_duration = response_metadata.get('total_duration', 0)\n",
    "    load_duration = response_metadata.get('load_duration', 0)\n",
    "    prompt_eval_duration = response_metadata.get('prompt_eval_duration', 0)\n",
    "    eval_duration = response_metadata.get('eval_duration', 0)\n",
    "    \n",
    "    # Extract token counts\n",
    "    prompt_eval_count = response_metadata.get('prompt_eval_count', 0)\n",
    "    eval_count = response_metadata.get('eval_count', 0)\n",
    "    \n",
    "    # Convert to seconds\n",
    "    total_seconds = total_duration / 1_000_000_000\n",
    "    load_seconds = load_duration / 1_000_000_000\n",
    "    prompt_eval_seconds = prompt_eval_duration / 1_000_000_000\n",
    "    eval_seconds = eval_duration / 1_000_000_000\n",
    "    \n",
    "    # tokens per second\n",
    "    tokens_per_second = eval_count / eval_seconds\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'generation_tokens_per_second': eval_count / eval_seconds if eval_seconds > 0 else 0,\n",
    "        'prompt_tokens_per_second': prompt_eval_count / prompt_eval_seconds if prompt_eval_seconds > 0 else 0,\n",
    "        'total_tokens': prompt_eval_count + eval_count,\n",
    "        'total_time_seconds': total_seconds,\n",
    "        'load_time_seconds': load_seconds,\n",
    "        'generation_time_seconds': eval_seconds,\n",
    "        'prompt_processing_time_seconds': prompt_eval_seconds,\n",
    "        }\n",
    "\n",
    "    print(f\"Total tokens: {metrics['total_tokens']}\")\n",
    "    print(f\"Total time seconds: {metrics['total_time_seconds']}\")\n",
    "    print(f\"Load time seconds: {metrics['load_time_seconds']}\")\n",
    "    print(f\"Generation time seconds: {metrics['generation_time_seconds']}\")\n",
    "    print(f\"Prompt processing time seconds: {metrics['prompt_processing_time_seconds']}\")\n",
    "    print(f\"Generation tokens per second: {metrics['generation_tokens_per_second']}\")\n",
    "    print(f\"Prompt tokens per second: {metrics['prompt_tokens_per_second']}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain quantum computing in one sentence.\n",
      "\n",
      "Generating response...\n",
      "\n",
      "✅ Response generated!\n",
      "\n",
      "Model output: Quantum computing harnesses qubits, which can be in superpositions of 0 and 1 and become entangled, to perform certain calculations exponentially faster than classical computers.\n"
     ]
    }
   ],
   "source": [
    "# Test simple inference\n",
    "prompt = \"Explain quantum computing in one sentence.\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nGenerating response...\")\n",
    "\n",
    "response = chat_model.invoke(prompt)\n",
    "\n",
    "print(f\"\\n✅ Response generated!\")\n",
    "print(f\"\\nModel output: {response.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 227\n",
      "Total time seconds: 29.951202208\n",
      "Load time seconds: 24.835582125\n",
      "Generation time seconds: 3.137161791\n",
      "Prompt processing time seconds: 1.95951225\n",
      "Generation tokens per second: 48.770197456481775\n",
      "Prompt tokens per second: 37.76449981366537\n"
     ]
    }
   ],
   "source": [
    "_ = detailed_performance_metrics(response.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending messages to model...\n",
      "\n",
      "✅ Response generated!\n",
      "\n",
      "Model output: **Machine learning (ML)** is a way to teach computers how to make predictions or decisions *without* being explicitly programmed for every single case.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. The Big Idea  \n",
      "- **Data → Patterns → Decisions**  \n",
      "  Think of a teacher showing a student many examples of cats and dogs. After seeing enough pictures, the student learns what a cat looks like and can guess correctly when shown a new picture.  \n",
      "  In ML, the computer is the student, the examples are the data, and the guessing is the prediction or decision.\n",
      "\n",
      "### 2. How It Works (in plain steps)\n",
      "\n",
      "| Step | What Happens | Analogy |\n",
      "|------|--------------|---------|\n",
      "| **Collect data** | Gather many examples (images, numbers, text, etc.). | A photo album of cats and dogs. |\n",
      "| **Choose a model** | Pick a mathematical “recipe” that can learn patterns (e.g., a neural network, decision tree). | Pick a notebook style (grid, free‑form). |\n",
      "| **Train** | Feed the data to the model so it adjusts its internal settings to reduce mistakes. | The student practices, gets feedback, and improves. |\n",
      "| **Validate / test** | Check the model on new data it hasn’t seen to see how well it generalizes. | The student takes a practice quiz. |\n",
      "| **Deploy** | Use the trained model in real life (e.g., spam filter, recommendation engine). | The student uses the knowledge in a real conversation. |\n",
      "\n",
      "### 3. Types of Learning\n",
      "\n",
      "| Type | Goal | Example |\n",
      "|------|------|---------|\n",
      "| **Supervised** | Learn from labeled data (input + correct output). | Predict house prices from features like size, location. |\n",
      "| **Unsupervised** | Find hidden structure in unlabeled data. | Group customers into segments based on buying habits. |\n",
      "| **Reinforcement** | Learn by trial‑and‑error, receiving rewards or penalties. | A robot learns to navigate a maze. |\n",
      "\n",
      "### 4. Why It Matters\n",
      "\n",
      "- **Automation**: Handles tasks too big or tedious for humans (e.g., scanning millions of emails for spam).  \n",
      "- **Personalization**: Recommends movies, products, or news based on your behavior.  \n",
      "- **Insight**: Finds patterns in data that humans might miss (e.g., medical diagnosis from imaging).  \n",
      "\n",
      "### 5. A Quick, Everyday Example\n",
      "\n",
      "1. **Problem**: You want an app that tells you if a photo is a cat.  \n",
      "2. **Data**: Upload 1,000 photos labeled “cat” or “not cat.”  \n",
      "3. **Model**: Use a convolutional neural network (a type of deep learning model).  \n",
      "4. **Training**: The app looks at each photo, tries to guess, learns from mistakes.  \n",
      "5. **Result**: After training, it can correctly identify cats in new photos 95% of the time.\n",
      "\n",
      "---\n",
      "\n",
      "**Bottom line**: Machine learning is like giving a computer a big set of examples, letting it learn patterns on its own, and then using that learning to make smart guesses or decisions in new situations.\n"
     ]
    }
   ],
   "source": [
    "# Test with system message and human message\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant that explains complex topics simply.\"),\n",
    "    HumanMessage(content=\"What is machine learning?\")\n",
    "]\n",
    "\n",
    "print(\"Sending messages to model...\")\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "print(f\"\\n✅ Response generated!\")\n",
    "print(f\"\\nModel output: {response.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 798\n",
      "Total time seconds: 15.658207042\n",
      "Load time seconds: 0.083827\n",
      "Generation time seconds: 15.311673125\n",
      "Prompt processing time seconds: 0.241825708\n",
      "Generation tokens per second: 46.10861231404455\n",
      "Prompt tokens per second: 380.43928729033223\n"
     ]
    }
   ],
   "source": [
    "_ = detailed_performance_metrics(response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Streaming Response\n",
    "\n",
    "Ollama supports streaming responses, which is useful for real-time applications:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Write a haiku about artificial intelligence.\n",
      "\n",
      "Streaming response:\n",
      "----------------------------------------\n",
      "Silicon mind hums,  \n",
      "data streams, rivers of thought,  \n",
      "thought born in code's glow.\n",
      "----------------------------------------\n",
      "\n",
      "✅ Streaming completed!\n"
     ]
    }
   ],
   "source": [
    "# Test streaming\n",
    "prompt = \"Write a haiku about artificial intelligence.\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nStreaming response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for chunk in chat_model.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"\\n✅ Streaming completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "If all the tests above passed, you're ready to use Ollama with LangChain! Here's what we tested:\n",
    "\n",
    "✅ **Embeddings**: \n",
    "- Created embeddings for single queries\n",
    "- Created embeddings for multiple documents\n",
    "- Verified embedding dimensions\n",
    "\n",
    "✅ **Model Inference**:\n",
    "- Simple text generation\n",
    "- Chat with system and human messages\n",
    "- Streaming responses\n",
    "- Integration with LangChain chains\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "If you encounter issues:\n",
    "\n",
    "1. **Model Not Found**: Pull the required models (`ollama pull <model-name>`)\n",
    "2. **Slow Performance**: Ollama models run on CPU by default. For better performance:\n",
    "   - Use smaller models for testing\n",
    "   - Consider GPU acceleration if available\n",
    "3. **Memory Issues**: Large models require significant RAM. Try smaller variants if needed.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now you're ready to proceed with the main RAG assignment using Ollama!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Production RAG (.venv)",
   "language": "python",
   "name": "production-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
