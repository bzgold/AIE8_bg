{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJXW_DgiSebM"
   },
   "source": [
    "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
    "\n",
    "In the following notebook we'll complete the following tasks:\n",
    "\n",
    "- \ud83e\udd1d Breakout Room #1:\n",
    "  1. Install required libraries\n",
    "  2. Set Environment Variables\n",
    "  3. Creating our Tool Belt\n",
    "  4. Creating Our State\n",
    "  5. Creating and Compiling A Graph!\n",
    "\n",
    "- \ud83e\udd1d Breakout Room #2:\n",
    "  1. Evaluating the LangGraph Application with LangSmith\n",
    "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
    "  3. LangGraph for the \"Patterns\" of GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djQ3nRAgoF67"
   },
   "source": [
    "# \ud83e\udd1d Breakout Room #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7pQDUhUnIo8"
   },
   "source": [
    "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
    "\n",
    "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
    "\n",
    "### Why Cycles?\n",
    "\n",
    "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
    "\n",
    "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
    "\n",
    "### Why LangGraph?\n",
    "\n",
    "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_fLDElOVoop"
   },
   "source": [
    "## Task 1:  Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wujPjGJuoPwg"
   },
   "source": [
    "## Task 2: Environment Variables\n",
    "\n",
    "We'll want to set our OpenAI, Tavily, and LangSmith API keys along with our LangSmith environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jdh8CoVWHRvs",
    "outputId": "3fa78560-393c-4ee5-b871-9886bf0d70f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jkla2fpx28QK",
    "outputId": "52d7ad22-fcb1-4abe-853b-216c55a12650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAVILY_API_KEY \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nv0glIDyHmRt",
    "outputId": "b69df90a-b4e1-4ddb-9de0-882d98b68ab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith API Key:  \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE8 - LangGraph - {uuid4().hex[0:8]}\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBRyQmEAVzua"
   },
   "source": [
    "## Task 3: Creating our Tool Belt\n",
    "\n",
    "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
    "\n",
    "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain-community/tree/main/libs/community) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
    "\n",
    "We'll leverage:\n",
    "\n",
    "- [Tavily Search Results](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/tavily_search/tool.py)\n",
    "- [Arxiv](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/arxiv/tool.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k6n_Dob2F46"
   },
   "source": [
    "#### \ud83c\udfd7\ufe0f Activity #1:\n",
    "\n",
    "Please add the tools to use into our toolbelt.\n",
    "\n",
    "> NOTE: Each tool in our toolbelt should be a method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "lAxaSvlfIeOg"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "tool_belt = [\n",
    "    tavily_tool,\n",
    "    ArxivQueryRun(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI-C669ZYVI5"
   },
   "source": [
    "### Model\n",
    "\n",
    "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
    "\n",
    "- OpenAI's GPT-3.5 and GPT-4\n",
    "- Anthropic's Claude\n",
    "- Google's Gemini\n",
    "\n",
    "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "QkNS8rNZJs4z"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ugkj3GzuZpQv"
   },
   "source": [
    "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "4OdMqFafZ_0V"
   },
   "outputs": [],
   "source": [
    "model = model.bind_tools(tool_belt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERzuGo6W18Lr"
   },
   "source": [
    "#### \u2753 Question #1:\n",
    "\n",
    "How does the model determine which tool to use?\n",
    "\n",
    "ANSWER\u2705\n",
    "\n",
    "The model determines which tool to use by considering both the user prompt, which is the question or instruction entered by the person, and the internal built-in prompts, which are behind-the-scenes instructions that guide how to interpret and act on the user\u2019s request. It\u2019s this coordination between what the user asks for and how the system is internally guided that allows the model to \u201cthink\u201d and decide the best tool to use.\n",
    "\n",
    "For example, if the prompt is \u201cWhat is the temperature in Austin right now?\u201d, the model recognizes that this is a real-time data request. It may choose a weather API tool to fetch current temperature data, because the user wants live, factual information. However, if the prompt is \u201cSummarize the latest news about Austin traffic\u201d, the model recognizes a text processing task. It may first use a news-fetching tool to collect articles and then pass that text to a summarization tool to condense it into a clear summary.\n",
    "\n",
    "These two example tasks are different because one requires retrieving factual data from an external source, while the other requires processing and understanding text before delivering a summary. The model decides which tools to use based on the type of task implied by the user prompt and the guidance from its internal instructions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_296Ub96Z_H8"
   },
   "source": [
    "## Task 4: Putting the State in Stateful\n",
    "\n",
    "Earlier we used this phrasing:\n",
    "\n",
    "`coordinated multi-actor and stateful applications`\n",
    "\n",
    "So what does that \"stateful\" mean?\n",
    "\n",
    "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
    "\n",
    "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
    "\n",
    "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
    "\n",
    "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
    "\n",
    "1. We initialize our state object:\n",
    "  - `{\"messages\" : []}`\n",
    "2. Our user submits a query to our application.\n",
    "  - New State: `HumanMessage(#1)`\n",
    "  - `{\"messages\" : [HumanMessage(#1)}`\n",
    "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
    "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
    "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
    "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mxL9b_NZKUdL"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWsMhfO9grLu"
   },
   "source": [
    "## Task 5: It's Graphing Time!\n",
    "\n",
    "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
    "\n",
    "Let's take a second to refresh ourselves about what a graph is in this context.\n",
    "\n",
    "Graphs, also called networks in some circles, are a collection of connected objects.\n",
    "\n",
    "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
    "\n",
    "Let's look at a simple graph.\n",
    "\n",
    "![image](https://i.imgur.com/2NFLnIc.png)\n",
    "\n",
    "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
    "\n",
    "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
    "\n",
    "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
    "\n",
    "Let's create some nodes and expand on our diagram.\n",
    "\n",
    "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "91flJWtZLUrl"
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "def call_model(state):\n",
    "  messages = state[\"messages\"]\n",
    "  response = model.invoke(messages)\n",
    "  return {\"messages\" : [response]}\n",
    "\n",
    "tool_node = ToolNode(tool_belt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bwR7MgWj3Wg"
   },
   "source": [
    "Now we have two total nodes. We have:\n",
    "\n",
    "- `call_model` is a node that will...well...call the model\n",
    "- `tool_node` is a node which can call a tool\n",
    "\n",
    "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vF4_lgtmQNo",
    "outputId": "a4384377-8f7a-415f-be1b-fee6169cb101"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x75bc73aafa10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "uncompiled_graph = StateGraph(AgentState)\n",
    "\n",
    "uncompiled_graph.add_node(\"agent\", call_model)\n",
    "uncompiled_graph.add_node(\"action\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8CjRlbVmRpW"
   },
   "source": [
    "Let's look at what we have so far:\n",
    "\n",
    "![image](https://i.imgur.com/md7inqG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaXHpPeSnOWC"
   },
   "source": [
    "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGCbaYqRnmiw",
    "outputId": "5351807c-2ac7-4316-a3a3-878abeacd114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x75bc73aafa10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncompiled_graph.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUsfGoSpoF9U"
   },
   "source": [
    "![image](https://i.imgur.com/wNixpJe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Q_pQgHmoW0M"
   },
   "source": [
    "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
    "\n",
    "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
    "\n",
    "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
    "\n",
    "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
    "\n",
    "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BZgb81VQf9o",
    "outputId": "73a07c15-5f0b-40f2-b033-38b57d056dd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x75bc73aafa10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def should_continue(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  if last_message.tool_calls:\n",
    "    return \"action\"\n",
    "\n",
    "  return END\n",
    "\n",
    "uncompiled_graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Cvhcf4jp0Ce"
   },
   "source": [
    "Let's visualize what this looks like.\n",
    "\n",
    "![image](https://i.imgur.com/8ZNwKI5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKCjWJCkrJb9"
   },
   "source": [
    "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvcgbHf1rIXZ",
    "outputId": "45d4bdd6-d6bb-4a1d-bb79-cad43c130bf2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x75bc73aafa10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncompiled_graph.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiWDwBQtrw7Z"
   },
   "source": [
    "Let's look at the final visualization.\n",
    "\n",
    "![image](https://i.imgur.com/NWO7usO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYqDpErlsCsu"
   },
   "source": [
    "All that's left to do now is to compile our workflow - and we're off!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zt9-KS8DpzNx"
   },
   "outputs": [],
   "source": [
    "simple_agent_graph = uncompiled_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhNWIwBL1W4Q"
   },
   "source": [
    "#### \u2753 Question #2:\n",
    "\n",
    "Is there any specific limit to how many times we can cycle?\n",
    "\n",
    "ANSWER \u2705\n",
    "There isn\u2019t a built-in limit to how many times a multi-agent system can cycle. While it could keep iterating as long as the system allows, this can be problematic because it may lead to infinite loops, wasted resources, and slower performance. Setting a limit is important to ensure the system operates efficiently, prevents excessive computation, and avoids getting stuck in repetitive cycles without useful results.\n",
    "\n",
    "\n",
    "If not, how could we impose a limit to the number of cycles?\n",
    "\n",
    "ANSWER \u2705\n",
    "There are multiple ways to limit the number of cycles, which can be used individually or in combination:\n",
    "\n",
    "    1.) Global Cycle Counter \u2013 Track the total number of system-wide cycles and set a maximum allowed number. Once this limit is reached, the system stops.\n",
    "    2.) Agent-Specific Iteration Caps \u2013 Each agent tracks its own execution count and stops once it reaches a defined threshold. Some agents may finish sooner than others, providing flexibility.\n",
    "    3.) Overall Run Time \u2013 Implement a time-based limit so the system stops after a specified duration, independent of the number of cycles.\n",
    "    4.) Specific Conditions \u2013 Stop cycles based on output criteria, such as if the results haven\u2019t changed significantly or if a specific goal hasn\u2019t been achieved.\n",
    "\n",
    "Overall, these mechanisms provide flexibility to ensure the system runs efficiently while preventing infinite loops or unnecessary computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEYcTShCsPaa"
   },
   "source": [
    "## Using Our Graph\n",
    "\n",
    "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
    "\n",
    "Let's try out a few examples to see how it fairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qn4n37PQRPII",
    "outputId": "5eeedfae-089d-496e-e71f-071939fa5832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='Technical professionals are using AI in various ways to enhance their work, including automating repetitive tasks, improving decision-making, analyzing large datasets, developing new products and services, and optimizing processes. They leverage AI for tasks such as machine learning model development, natural language processing, computer vision, predictive analytics, and automation. This integration helps increase efficiency, accuracy, and innovation across different industries. Would you like specific examples from particular fields or recent trends?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 163, 'total_tokens': 252, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CJ4fyo1Z1IWLdKFwj8rgr8nhJWIHh', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ad0384e3-e732-45ea-8af9-8de660cabc8a-0', usage_metadata={'input_tokens': 163, 'output_tokens': 89, 'total_tokens': 252, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=\"How are technical professionals using AI to improve their work?\")]}\n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        print(values[\"messages\"])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBHnUtLSscRr"
   },
   "source": [
    "Let's look at what happened:\n",
    "\n",
    "1. Our state object was populated with our request\n",
    "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
    "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
    "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
    "5. The agent node added a response to the state object and passed it along the conditional edge\n",
    "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
    "\n",
    "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afv2BuEsV5JG",
    "outputId": "ff009536-d281-4a56-c126-9cd245352bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_eWPkp6AZXUZ8Ddv5RSccsgOD', 'function': {'arguments': '{\"query\": \"A Comprehensive Survey of Deep Research\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_inftH8lgkIIRT3PGEwdzfSTn', 'function': {'arguments': '{\"query\": \"author of A Comprehensive Survey of Deep Research\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 182, 'total_tokens': 242, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CJ4iCWQU8LgioK1yxump6CTPubRVg', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--72e6fe53-5b06-4e6d-9720-ebece80d8ef3-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'A Comprehensive Survey of Deep Research'}, 'id': 'call_eWPkp6AZXUZ8Ddv5RSccsgOD', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'author of A Comprehensive Survey of Deep Research'}, 'id': 'call_inftH8lgkIIRT3PGEwdzfSTn', 'type': 'tool_call'}], usage_metadata={'input_tokens': 182, 'output_tokens': 60, 'total_tokens': 242, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'action'\n",
      "Tool Used: arxiv\n",
      "[ToolMessage(content='Published: 2025-06-14\\nTitle: A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications\\nAuthors: Renjun Xu, Jingwen Peng\\nSummary: This survey examines the rapidly evolving field of Deep Research systems --\\nAI-powered applications that automate complex research workflows through the\\nintegration of large language models, advanced information retrieval, and\\nautonomous reasoning capabilities. We analyze more than 80 commercial and\\nnon-commercial implementations that have emerged since 2023, including\\nOpenAI/Deep Research, Gemini/Deep Research, Perplexity/Deep Research, and\\nnumerous open-source alternatives. Through comprehensive examination, we\\npropose a novel hierarchical taxonomy that categorizes systems according to\\nfour fundamental technical dimensions: foundation models and reasoning engines,\\ntool utilization and environmental interaction, task planning and execution\\ncontrol, and knowledge synthesis and output generation. We explore the\\narchitectural patterns, implementation approaches, and domain-specific\\nadaptations that characterize these systems across academic, scientific,\\nbusiness, and educational applications. Our analysis reveals both the\\nsignificant capabilities of current implementations and the technical and\\nethical challenges they present regarding information accuracy, privacy,\\nintellectual property, and accessibility. The survey concludes by identifying\\npromising research directions in advanced reasoning architectures, multimodal\\nintegration, domain specialization, human-AI collaboration, and ecosystem\\nstandardization that will likely shape the future evolution of this\\ntransformative technology. By providing a comprehensive framework for\\nunderstanding Deep Research systems, this survey contributes to both the\\ntheoretical understanding of AI-augmented knowledge work and the practical\\ndevelopment of more capable, responsible, and accessible research technologies.\\nThe paper resources can be viewed at\\nhttps://github.com/scienceaix/deepresearch.\\n\\nPublished: 2021-03-05\\nTitle: A comprehensive survey on point cloud registration\\nAuthors: Xiaoshui Huang, Guofeng Mei, Jian Zhang, Rana Abbas\\nSummary: Registration is a transformation estimation problem between two point clouds,\\nwhich has a unique and critical role in numerous computer vision applications.\\nThe developments of optimization-based methods and deep learning methods have\\nimproved registration robustness and efficiency. Recently, the combinations of\\noptimization-based and deep learning methods have further improved performance.\\nHowever, the connections between optimization-based and deep learning methods\\nare still unclear. Moreover, with the recent development of 3D sensors and 3D\\nreconstruction techniques, a new research direction emerges to align\\ncross-source point clouds. This survey conducts a comprehensive survey,\\nincluding both same-source and cross-source registration methods, and summarize\\nthe connections between optimization-based and deep learning methods, to\\nprovide further research insight. This survey also builds a new benchmark to\\nevaluate the state-of-the-art registration algorithms in solving cross-source\\nchallenges. Besides, this survey summarizes the benchmark data sets and\\ndiscusses point cloud registration applications across various domains.\\nFinally, this survey proposes potential research directions in this rapidly\\ngrowing field.\\n\\nPublished: 2023-07-07\\nTitle: A Survey of Deep Learning in Sports Applications: Perception, Comprehension, and Decision\\nAuthors: Zhonghan Zhao, Wenhao Chai, Shengyu Hao, Wenhao Hu, Guanhong Wang, Shidong Cao, Mingli Song, Jenq-Neng Hwang, Gaoang Wang\\nSummary: Deep learning has the potential to revolutionize sports performance, with\\napplications ranging from perception and comprehension to decision. This paper\\npresents a comprehensive survey of deep learning in sports performance,\\nfocusing on three main aspects: algorithms, datasets and virtual environments,\\nand challenges. Firstly, we discuss th', name='arxiv', id='8588494c-a976-4980-8ab4-8c638c09515b', tool_call_id='call_eWPkp6AZXUZ8Ddv5RSccsgOD'), ToolMessage(content='[{\"title\": \"[2506.12594] A Comprehensive Survey of Deep Research - arXiv\", \"url\": \"https://arxiv.org/abs/2506.12594\", \"content\": \"We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\\\\n\\\\n> cs > arXiv:2506.12594\\\\n\\\\n# Computer Science > Artificial Intelligence\\\\n\\\\narXiv:2506.12594 (cs)\\\\n\\\\n[Submitted on 14 Jun 2025]\\\\n\\\\n# Title:A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications\\\\n\\\\nAuthors:Renjun Xu, Jingwen Peng [...] View a PDF of the paper titled A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications, by Renjun Xu and 1 other authors [...] View a PDF of the paper titled A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications, by Renjun Xu and 1 other authors\\\\n\\\\n View PDF\\\\n HTML (experimental)\\\\n TeX Source\\\\n Other Formats\\\\n\\\\nview license\\\\n\\\\nCurrent browse context:\\\\n\\\\ncs.AI\\\\n\\\\n< prev\\\\\")    |    next >\\\\\")\\\\n\\\\nnew  |  recent  | 2025-06\\\\n\\\\nChange to browse by:\\\\n\\\\ncs cs.MA\\\\n\\\\n### References & Citations\\\\n\\\\n NASA ADS\\\\n Google Scholar\\\\n Semantic Scholar\\\\n\\\\na export BibTeX citation Loading...\\\\n\\\\n## BibTeX formatted citation\\\\n\\\\n\u00d7\", \"score\": 0.90289277}, {\"title\": \"A comprehensive survey of deep learning research on medical ...\", \"url\": \"https://pubmed.ncbi.nlm.nih.gov/36462229/\", \"content\": \"PMID: 36462229\\\\n    DOI: 10.1016/j.clinimag.2022.11.003\\\\n\\\\n Item in Clipboard \\\\n\\\\nReview\\\\n\\\\nA comprehensive survey of deep learning research on medical image analysis with focus on transfer learning\\\\n\\\\nSema Atasever et al. Clin Imaging.2023 Feb.\\\\n\\\\nShow details\\\\n\\\\nDisplay options\\\\n\\\\n Display options \\\\n\\\\n Format \\\\n\\\\n Clin Imaging \\\\n\\\\nActions\\\\n\\\\n   Search in PubMed\\\\n   Search in NLM Catalog\\\\n   Add to Search\\\\n\\\\n. 2023 Feb:94:18-41.\\\\n\\\\n doi: 10.1016/j.clinimag.2022.11.003.  Epub 2022 Nov 12. \\\\n\\\\n### Authors [...] doi: 10.1016/j.clinimag.2022.11.003.  Epub 2022 Nov 12. \\\\n\\\\nA comprehensive survey of deep learning research on medical image analysis with focus on transfer learning\\\\n\\\\nSema Atasever1,Nuh Azginoglu2,Duygu Sinanc Terzi3,Ramazan Terzi4\\\\n\\\\n Affiliations  Expand \\\\n\\\\n### Affiliations [...] A Deep Convolutional Neural Network for Pneumonia Detection in X-ray Images with Attention Ensemble.An Q, Chen W, Shao W.An Q, et al.Diagnostics (Basel). 2024 Feb 11;14(4):390. doi: 10.3390/diagnostics14040390.Diagnostics (Basel). 2024.PMID: 38396430 Free PMC article.\", \"score\": 0.71425}, {\"title\": \"A Comprehensive Survey of Deep Learning Approaches in Image ...\", \"url\": \"https://www.mdpi.com/1424-8220/25/2/531\", \"content\": \"Elias Dritsas\\\\n\\\\nElias Dritsas\\\\n\\\\nSciProfiles  Scilit  Preprints.org  Google Scholar\\\\n\\\\n \\\\\\\\\\\\n\\\\nIndustrial Systems Institute (ISI), Athena Research and Innovation Center, 26504 Patras, Greece\\\\n\\\\n\\\\\\\\\\\\n\\\\nAuthor to whom correspondence should be addressed.\\\\n\\\\nSensors 2025, 25(2), 531; \\\\n\\\\nSubmission received: 20 December 2024 / Revised: 13 January 2025 / Accepted: 13 January 2025 / Published: 17 January 2025 [...] Visit our dedicated information section to learn more about MDPI.\\\\n\\\\n Get Information\\\\n\\\\nclear\\\\n\\\\n## JSmol Viewer\\\\n\\\\nclear\\\\n\\\\nfirst\\\\\\\\_page\\\\n\\\\n Download PDF \\\\n\\\\nsettings\\\\n\\\\n Order Article Reprints\\\\n\\\\nFont Type:\\\\n\\\\nArial Georgia Verdana\\\\n\\\\nFont Size:\\\\n\\\\nAa Aa Aa\\\\n\\\\nLine Spacing:\\\\n\\\\n\\uf034   \\uf034   \\uf034\\\\n\\\\nColumn Width:\\\\n\\\\n\\uf035   \\uf035   \\uf035\\\\n\\\\nBackground:\\\\n\\\\nOpen AccessReview\\\\n\\\\n# A Comprehensive Survey of Deep Learning Approaches in Image Processing\\\\n\\\\nby \\\\n\\\\nMaria Trigka\\\\n\\\\nMaria Trigka\\\\n\\\\nSciProfiles  Scilit  Preprints.org  Google Scholar\\\\n\\\\nand [...] transferring irrelevant knowledge, ensuring effective domain alignment. Apart from its contribution to negative transfer mitigation, it also enhances fine-grained feature extraction, addresses the scarcity of labeled data with self-supervised pre-training, and resolves class imbalance using key point sensitive loss. These strategies demonstrate the solution\u2019s robustness in sonar image classification challenges. A summary of topics discussed regarding transfer learning techniques is presented in\", \"score\": 0.6469293}, {\"title\": \"[PDF] A Comprehensive Survey of Deep Research - arXiv\", \"url\": \"https://arxiv.org/pdf/2506.12594?\", \"content\": \"> (2023 - February 2025) Early prototypes and foundational approaches (February - March 2025) Commercial releases and competitive rivalry (March 2025 - Present) Multi-modal integration and diverse applications\\\\n> Google Gemini Deep Research\\\\n> Dec 2024\\\\n> OpenAI Deep Research\\\\n> Feb 2025\\\\n> Manus\\\\n> Mar 2025\\\\n> Perplexity Deep Research\\\\n> Feb 2025\\\\n> AutoGLM-Research\\\\n> Mar 2025\\\\n> QwenLM/ Qwen-Agent\\\\n> Apr 2024\\\\n> n8n\\\\n> 2023 mshumer/ OpenDeepResearcher\\\\n> Feb 2025\\\\n> nickscamara/ open-deep-research [...] The OpenAI/AgentsSDK [199 ] demonstrates sophisticated planning capabilities with hierarchical task decomposition and adaptive execution, enabling complex research workflows with reliable completion rates. Similarly, Flowith/OracleMode [77 ] offers advanced planning mechanisms optimized for research tasks, though with more limited error recovery capabilities. 16 Xu et al. [...] Human-AI Collaboration and Standardization 72 9 Conclusion 76 9.1 Key Findings and Contributions 76 9.2 Limitations and Outlook 78 9.3 Broader Implications 79 9.4 Final Thoughts 80 References 81 4 Xu et al.\", \"score\": 0.6453216}, {\"title\": \"A Comprehensive Survey of Deep Learning Applications in Big Data ...\", \"url\": \"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5005162\", \"content\": \"Permalink\\\\n\\\\nUsing these links will ensure access to this page indefinitely\\\\n\\\\nCopy URL\\\\n\\\\nCopy DOI\\\\n\\\\nA Comprehensive Survey of Deep Learning Applications in Big Data Analytics: Trends, Techniques, and Future Directions\\\\n\\\\n27 Pages Posted: 19 Nov 2024\\\\n\\\\nSee all articles by Ayaz H. Khan\\\\nAyaz H. Khan\\\\n\\\\n_affiliation not provided to SSRN_\\\\n\\\\n### Abstract [...] A Comprehensive Survey of Deep Learning Applications in Big Data Analytics: Trends, Techniques, and Future Directions by Ayaz H. Khan :: SSRN\\\\n\\\\nSkip to main content\\\\n\\\\nImage 1\\\\n\\\\nImage 2\\\\n\\\\nMake use of personalized features like alerts and saved searches\\\\n\\\\nCreate accountSign in\\\\n\\\\n   Product & Services\\\\n\\\\n       Research Paper Series\\\\n       Site Subscriptions\\\\n       Sponsored Services\\\\n       Jobs & Announcements\\\\n       Conference Papers\\\\n       Partners in Publishing\\\\n       First Look [...] Keywords: Deep learning, Big Data Analytics, Convolutional neural networks, recurrent neural networks, Long short-term memory\\\\n\\\\nSuggested Citation:Suggested Citation\\\\n\\\\n Khan, Ayaz H., A Comprehensive Survey of Deep Learning Applications in Big Data Analytics: Trends, Techniques, and Future Directions. Available at SSRN:  or \\\\n\\\\n\\\\n\\\\n#### _affiliation not provided to SSRN_ ( email) )\\\\n\\\\nImage 5: PDF iconDownload This Paper\\\\n\\\\nOpen PDF in Browser\\\\n\\\\n### 0 References\\\\n\\\\nFetch References\\\\n\\\\n### 0 Citations\", \"score\": 0.5737984}]', name='tavily_search_results_json', id='812bf7bb-fafd-49e3-92fc-2a68c0abf2d8', tool_call_id='call_inftH8lgkIIRT3PGEwdzfSTn', artifact={'query': 'author of A Comprehensive Survey of Deep Research', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://arxiv.org/abs/2506.12594', 'title': '[2506.12594] A Comprehensive Survey of Deep Research - arXiv', 'content': 'We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\\n\\n> cs > arXiv:2506.12594\\n\\n# Computer Science > Artificial Intelligence\\n\\narXiv:2506.12594 (cs)\\n\\n[Submitted on 14 Jun 2025]\\n\\n# Title:A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications\\n\\nAuthors:Renjun Xu, Jingwen Peng [...] View a PDF of the paper titled A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications, by Renjun Xu and 1 other authors [...] View a PDF of the paper titled A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications, by Renjun Xu and 1 other authors\\n\\n View PDF\\n HTML (experimental)\\n TeX Source\\n Other Formats\\n\\nview license\\n\\nCurrent browse context:\\n\\ncs.AI\\n\\n< prev\")    |    next >\")\\n\\nnew  |  recent  | 2025-06\\n\\nChange to browse by:\\n\\ncs cs.MA\\n\\n### References & Citations\\n\\n NASA ADS\\n Google Scholar\\n Semantic Scholar\\n\\na export BibTeX citation Loading...\\n\\n## BibTeX formatted citation\\n\\n\u00d7', 'score': 0.90289277, 'raw_content': None}, {'url': 'https://pubmed.ncbi.nlm.nih.gov/36462229/', 'title': 'A comprehensive survey of deep learning research on medical ...', 'content': 'PMID: 36462229\\n    DOI: 10.1016/j.clinimag.2022.11.003\\n\\n Item in Clipboard \\n\\nReview\\n\\nA comprehensive survey of deep learning research on medical image analysis with focus on transfer learning\\n\\nSema Atasever et al. Clin Imaging.2023 Feb.\\n\\nShow details\\n\\nDisplay options\\n\\n Display options \\n\\n Format \\n\\n Clin Imaging \\n\\nActions\\n\\n   Search in PubMed\\n   Search in NLM Catalog\\n   Add to Search\\n\\n. 2023 Feb:94:18-41.\\n\\n doi: 10.1016/j.clinimag.2022.11.003.  Epub 2022 Nov 12. \\n\\n### Authors [...] doi: 10.1016/j.clinimag.2022.11.003.  Epub 2022 Nov 12. \\n\\nA comprehensive survey of deep learning research on medical image analysis with focus on transfer learning\\n\\nSema Atasever1,Nuh Azginoglu2,Duygu Sinanc Terzi3,Ramazan Terzi4\\n\\n Affiliations  Expand \\n\\n### Affiliations [...] A Deep Convolutional Neural Network for Pneumonia Detection in X-ray Images with Attention Ensemble.An Q, Chen W, Shao W.An Q, et al.Diagnostics (Basel). 2024 Feb 11;14(4):390. doi: 10.3390/diagnostics14040390.Diagnostics (Basel). 2024.PMID: 38396430 Free PMC article.', 'score': 0.71425, 'raw_content': None}, {'url': 'https://www.mdpi.com/1424-8220/25/2/531', 'title': 'A Comprehensive Survey of Deep Learning Approaches in Image ...', 'content': 'Elias Dritsas\\n\\nElias Dritsas\\n\\nSciProfiles  Scilit  Preprints.org  Google Scholar\\n\\n \\\\\\n\\nIndustrial Systems Institute (ISI), Athena Research and Innovation Center, 26504 Patras, Greece\\n\\n\\\\\\n\\nAuthor to whom correspondence should be addressed.\\n\\nSensors 2025, 25(2), 531; \\n\\nSubmission received: 20 December 2024 / Revised: 13 January 2025 / Accepted: 13 January 2025 / Published: 17 January 2025 [...] Visit our dedicated information section to learn more about MDPI.\\n\\n Get Information\\n\\nclear\\n\\n## JSmol Viewer\\n\\nclear\\n\\nfirst\\\\_page\\n\\n Download PDF \\n\\nsettings\\n\\n Order Article Reprints\\n\\nFont Type:\\n\\nArial Georgia Verdana\\n\\nFont Size:\\n\\nAa Aa Aa\\n\\nLine Spacing:\\n\\n\\uf034   \\uf034   \\uf034\\n\\nColumn Width:\\n\\n\\uf035   \\uf035   \\uf035\\n\\nBackground:\\n\\nOpen AccessReview\\n\\n# A Comprehensive Survey of Deep Learning Approaches in Image Processing\\n\\nby \\n\\nMaria Trigka\\n\\nMaria Trigka\\n\\nSciProfiles  Scilit  Preprints.org  Google Scholar\\n\\nand [...] transferring irrelevant knowledge, ensuring effective domain alignment. Apart from its contribution to negative transfer mitigation, it also enhances fine-grained feature extraction, addresses the scarcity of labeled data with self-supervised pre-training, and resolves class imbalance using key point sensitive loss. These strategies demonstrate the solution\u2019s robustness in sonar image classification challenges. A summary of topics discussed regarding transfer learning techniques is presented in', 'score': 0.6469293, 'raw_content': None}, {'url': 'https://arxiv.org/pdf/2506.12594?', 'title': '[PDF] A Comprehensive Survey of Deep Research - arXiv', 'content': '> (2023 - February 2025) Early prototypes and foundational approaches (February - March 2025) Commercial releases and competitive rivalry (March 2025 - Present) Multi-modal integration and diverse applications\\n> Google Gemini Deep Research\\n> Dec 2024\\n> OpenAI Deep Research\\n> Feb 2025\\n> Manus\\n> Mar 2025\\n> Perplexity Deep Research\\n> Feb 2025\\n> AutoGLM-Research\\n> Mar 2025\\n> QwenLM/ Qwen-Agent\\n> Apr 2024\\n> n8n\\n> 2023 mshumer/ OpenDeepResearcher\\n> Feb 2025\\n> nickscamara/ open-deep-research [...] The OpenAI/AgentsSDK [199 ] demonstrates sophisticated planning capabilities with hierarchical task decomposition and adaptive execution, enabling complex research workflows with reliable completion rates. Similarly, Flowith/OracleMode [77 ] offers advanced planning mechanisms optimized for research tasks, though with more limited error recovery capabilities. 16 Xu et al. [...] Human-AI Collaboration and Standardization 72 9 Conclusion 76 9.1 Key Findings and Contributions 76 9.2 Limitations and Outlook 78 9.3 Broader Implications 79 9.4 Final Thoughts 80 References 81 4 Xu et al.', 'score': 0.6453216, 'raw_content': None}, {'url': 'https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5005162', 'title': 'A Comprehensive Survey of Deep Learning Applications in Big Data ...', 'content': 'Permalink\\n\\nUsing these links will ensure access to this page indefinitely\\n\\nCopy URL\\n\\nCopy DOI\\n\\nA Comprehensive Survey of Deep Learning Applications in Big Data Analytics: Trends, Techniques, and Future Directions\\n\\n27 Pages Posted: 19 Nov 2024\\n\\nSee all articles by Ayaz H. Khan\\nAyaz H. Khan\\n\\n_affiliation not provided to SSRN_\\n\\n### Abstract [...] A Comprehensive Survey of Deep Learning Applications in Big Data Analytics: Trends, Techniques, and Future Directions by Ayaz H. Khan :: SSRN\\n\\nSkip to main content\\n\\nImage 1\\n\\nImage 2\\n\\nMake use of personalized features like alerts and saved searches\\n\\nCreate accountSign in\\n\\n   Product & Services\\n\\n       Research Paper Series\\n       Site Subscriptions\\n       Sponsored Services\\n       Jobs & Announcements\\n       Conference Papers\\n       Partners in Publishing\\n       First Look [...] Keywords: Deep learning, Big Data Analytics, Convolutional neural networks, recurrent neural networks, Long short-term memory\\n\\nSuggested Citation:Suggested Citation\\n\\n Khan, Ayaz H., A Comprehensive Survey of Deep Learning Applications in Big Data Analytics: Trends, Techniques, and Future Directions. Available at SSRN:  or \\n\\n\\n\\n#### _affiliation not provided to SSRN_ ( email) )\\n\\nImage 5: PDF iconDownload This Paper\\n\\nOpen PDF in Browser\\n\\n### 0 References\\n\\nFetch References\\n\\n### 0 Citations', 'score': 0.5737984, 'raw_content': None}], 'response_time': 2.08, 'request_id': '0549fc9b-c7b6-46cb-bad4-df50d299f167'})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='I found the research paper titled \"A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications\" authored by Renjun Xu and Jingwen Peng, published on June 14, 2025. \\n\\nRegarding the authors\\' current affiliations, I will now search each of them to find out where they work now.', additional_kwargs={'tool_calls': [{'id': 'call_YuUb5gSQ3XdcmGCiVa5O5u1b', 'function': {'arguments': '{\"query\": \"Renjun Xu\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_onKlxlSCDfP3tsjIWu4q1do5', 'function': {'arguments': '{\"query\": \"Jingwen Peng\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 123, 'prompt_tokens': 2832, 'total_tokens': 2955, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CJ4iHofWDBq2Vf7OMIYGCKqkwmfqx', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8e7c0341-b158-4e23-adf0-eeeffacc688a-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Renjun Xu'}, 'id': 'call_YuUb5gSQ3XdcmGCiVa5O5u1b', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Jingwen Peng'}, 'id': 'call_onKlxlSCDfP3tsjIWu4q1do5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2832, 'output_tokens': 123, 'total_tokens': 2955, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'action'\n",
      "Tool Used: tavily_search_results_json\n",
      "[ToolMessage(content='[{\"title\": \"Renjun Xu - Researcher, Zhejiang University - OpenReview\", \"url\": \"https://openreview.net/profile?id=~Renjun_Xu1\", \"content\": \"# Renjun Xu\\\\n\\\\n### Principal Researcher, Zhejiang University\\\\n\\\\n#### Names\\\\n\\\\n#### Emails\\\\n\\\\n#### Personal Links\\\\n\\\\n#### Career & Education History\\\\n\\\\n#### Advisors, Relations & Conflicts\\\\n\\\\nNo relations added\\\\n\\\\n#### Expertise\\\\n\\\\n#### Publications\\\\n\\\\n#### scKGOT: Intercellular Signaling Inference with Knowledge Graph Optimal Transport for Single-cell Transcriptomics)\\\\n\\\\n#### $E(2)$-Equivariant Vision Transformer)\\\\n\\\\n#### Critical Temperature Prediction of Superconductors Based on Machine Learning: A Short Review) [...] #### Exploiting Adapters for Cross-Lingual Low-Resource Speech Recognition)\\\\n\\\\n#### Hierarchical knowledge amalgamation with dual discriminative feature alignment)\\\\n\\\\n#### Modeling Dynamic Missingness of Implicit Feedback for Sequential Recommendation)\\\\n\\\\n#### S2SNet: A Pretrained Neural Network for Superconductivity Discovery)\\\\n\\\\n#### Learning Interest-oriented Universal User Representation via Self-supervision) [...] #### Learning Universal User Representations via Self-Supervised Lifelong Behaviors Modeling)\\\\n\\\\n#### Learning Invariant Representations across Domains and Tasks)\\\\n\\\\n#### Co-Authors\\\\n\\\\nOpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors. \u00a9 2025 OpenReview\", \"score\": 0.64047897}, {\"title\": \"Renjun Xu - Center for Data Science, Zhejiang University | \u4eba\u624d\u753b\u50cf\", \"url\": \"https://www.aminer.cn/profile/renjun-xu/53f42ceddabfaedd74d30355?source=bz1\", \"content\": \"Renjun Xu - Center for Data Science, Zhejiang University | \u4eba\u624d\u753b\u50cf - AMiner\\\\n\\\\n\\\\n\\\\nResearch\\\\n\\\\nCenter for Data Science Zhejiang University\\\\n\\\\n\u3001\u300aInternational Joint Conference on Artificial Intelligence\u300b(IJCAI, CCF-A), \u300aIEEE Transactions on Knowledge and Data Engineering\u300b(TKDE, CCF-A)\u4ea4\u53c9\u9886\u57df\u53d1\u8868\u591a\u7bc7\u56fd\u9645\u9876\u5c16\u671f\u520a\u548c\u4f1a\u8bae\u6587\u7ae0\uff0cCVPR\u3001AAAI\u3001NIPS\u3001TPAMI\u3001TIP\u3001TLT\u7b49\u9876\u7ea7\u4eba\u5de5\u667a\u80fd\u671f\u520a\u548c\u4f1a\u8bae\u7a0b\u5e8f\u59d4\u5458\u4f1a\u59d4\u5458\uff0c\u8363\u83b72020\u5e74\u5ea6\u4e16\u754c\u4eba\u5de5\u667a\u80fd\u5927\u4f1a\u9752\u5e74\u4f18\u79c0\u8bba\u6587\u63d0\u540d\u5956\uff0c\u6307\u5bfc\u5e76\u63a8\u8350\u7684\u6240\u6709\u5b66\u751f\u5747\u5df2\u62ff\u5230\u9ebb\u7701\u7406\u5de5\u5b66\u9662(MIT)\u3001\u5361\u5185\u57fa\u6885\u9686\u5927\u5b66(CMU)\u7b49\u5168\u7403\u9876\u5c16\u540d\u6821\u7684offer\uff01\\\\n\\\\nEducation\\\\n\\\\nSign in to view more\\\\n\\\\nExperience\\\\n\\\\nSign in to view more [...] Research Interests\\\\n\\\\n2012 2025\\\\n\\\\nPapers \u5171 39 \u7bc7 Patents \u5171 9 \u7bc7 Author Statistics Co-Author Similar Experts\\\\n\\\\nBy Year By Citation \u4e3b\u9898\u7b5b\u9009 \u671f\u520a\u7ea7\u522b\u7b5b\u9009 \u5408\u4f5c\u8005\u7b5b\u9009 \u5408\u4f5c\u673a\u6784\u7b5b\u9009\\\\n\\\\n\u65f6\u95f4\\\\n\\\\n\u5f15\u7528\u91cf\\\\n\\\\n\u4e3b\u9898\\\\n\\\\n\u671f\u520a\u7ea7\u522b\\\\n\\\\n\u5408\u4f5c\u8005\\\\n\\\\n\u5408\u4f5c\u673a\u6784\\\\n\\\\nAll 2025 2024 2023 2022 2021 2020 2015 2014 2013 2012 2010 2006\\\\n\\\\nDo PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning Vs. Memorization in Large Language Models\\\\n\\\\nYang Yan,Yu Lu,Renjun Xu,Zhenzhong Lan\\\\n\\\\narXiv \u00b7 Computation and Language\uff082025\uff09\\\\n\\\\nCited 0 Views 11 Bibtex\\\\n\\\\n0\\\\n\\\\n11 [...] The page data are from open Internet sources, cooperative publishers and automatic analysis results through AI technology. We do not make any commitments and guarantees for the validity, accuracy, correctness, reliability, completeness and timeliness of the page data. If you have any questions, please contact us by email: report@aminer.cn\\\\n\\\\nSwipe to Fine Result\", \"score\": 0.60049355}, {\"title\": \"Renjun Hu\\'s Homepage\", \"url\": \"https://hurenjun.github.io/\", \"content\": \"an algorithm engineer at Alibaba Cloud, contributing to AI-driven transformations across various business\\\\ndomains including feed recommendation, user growth, online marketing, and LLM-as-a-Judge.\\\\nSince January 2025, he has joined the School of Data Science and Engineering, East China Normal University\\\\nas a young researcher.\\\\nHis recent research interests include robust machine learning and the understanding, evaluation, and applications of large language models. [...] Renjun Hu received his Bachelor\\'s degree in 2014 and Ph.D. in 2020 from the School of Computer Science\\\\nand Engineering at Beihang University. From September 2017 to April 2018, he was a joint Ph.D. student\\\\nin the Data Mining Group at Rutgers University. He then worked as a research intern at the Business\\\\nIntelligence Lab of Baidu Research from May 2018 to September 2019. During 2020 to 2024, he served as [...] Renjun\\'s avatar\\\\n\\\\n### Renjun Hu     (\u80e1\u4ec1\u541b)\\\\n\\\\n#### Young Researcher\\\\n\\\\nSchool of Data Science of Engineering (DaSE)  \\\\nEast China Normal University (ECNU)\\\\n\\\\n \\\\nRoom X109, Shuxueguan, Putuo Campus\\\\n\\\\n \\\\nrjhu [at] dase.ecnu.edu.cn    renjun0hu [at] gmail.com\\\\n\\\\n### Short Bio\", \"score\": 0.6002124}, {\"title\": \"Chinese Government Intelligence Officer Sentenced to 20 ...\", \"url\": \"https://www.justice.gov/archives/opa/pr/chinese-government-intelligence-officer-sentenced-20-years-prison-espionage-crimes-attempting\", \"content\": \"On Nov. 5, 2021, a federal jury in Cincinnati convicted Xu on all counts: conspiracy to commit economic espionage, conspiracy to commit trade secret theft, attempted economic espionage and attempted trade secret theft.\\\\n\\\\nXu was a career intelligence officer, beginning in 2003 and rising to the rank of deputy division director at the Chinese Ministry of State Security (MSS), the intelligence and security agency for China. [...] \u201cThis case sends a clear message: we will hold accountable anyone attempting to steal American trade secrets,\u201d said U.S. Attorney Kenneth L. Parker for the Southern District of Ohio. \u201cXu conspired to steal American science and technology. Thanks to the diligent work of the FBI, GE Aviation and our trial team, he\u2019ll spend decades in federal prison.\u201d [...] According to court documents and trial testimony, beginning in at least December 2013, Xu targeted specific companies in the United States and abroad that are recognized as leaders in the field of aviation.\", \"score\": 0.43770394}, {\"title\": \"Xu Genjun - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Xu_Genjun\", \"content\": \"Xu was born in She County, Anhui on 23 November 1935. He graduated from the Chemistry Department of Fudan University in 1957, and joined the Shanghai Institute of Biochemistry afterwards, where he participated in and made significant contributions to the insulin synthesis project led by Wang Yinglai. He later researched enzyme kinetics, chemical modification of proteins, the relationship between proteins\\' structure and function, and the folding and refolding of proteins. [...] Xu published more than 100 research papers in academic journals. He was a two-time winner of the State Natural Science Award (first class), in addition to several national prizes by the Chinese Academy of Sciences (CAS). He was also awarded the Ho Leung Ho Lee Prize for Life Sciences. He was elected as an academician of the CAS in 1991.\\\\n\\\\nXu died on 8 January 2008 at Zhongshan Hospital in Shanghai, at the age of 72.\\\\n\\\\n## References\\\\n\\\\nWikimedia Foundation\\\\nPowered by MediaWiki [...] Wikipedia\\\\nThe Free Encyclopedia\\\\n\\\\n## Contents\\\\n\\\\n# Xu Genjun\\\\n\\\\nXu Genjun (Chinese: \u8bb8\u6839\u4fca; 23 November 1935 \u2013 8 January 2008) was a Chinese biochemist. He was a professor at the Shanghai Institute of Biochemistry and Cell Biology. He was an academician of the Chinese Academy of Sciences and President of the Chinese Society of Biochemistry and Molecular Biology.\\\\n\\\\n## Biography\", \"score\": 0.25395855}]', name='tavily_search_results_json', id='558e4d0f-56d8-4e5b-8227-95e482d218dc', tool_call_id='call_YuUb5gSQ3XdcmGCiVa5O5u1b', artifact={'query': 'Renjun Xu', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://openreview.net/profile?id=~Renjun_Xu1', 'title': 'Renjun Xu - Researcher, Zhejiang University - OpenReview', 'content': '# Renjun Xu\\n\\n### Principal Researcher, Zhejiang University\\n\\n#### Names\\n\\n#### Emails\\n\\n#### Personal Links\\n\\n#### Career & Education History\\n\\n#### Advisors, Relations & Conflicts\\n\\nNo relations added\\n\\n#### Expertise\\n\\n#### Publications\\n\\n#### scKGOT: Intercellular Signaling Inference with Knowledge Graph Optimal Transport for Single-cell Transcriptomics)\\n\\n#### $E(2)$-Equivariant Vision Transformer)\\n\\n#### Critical Temperature Prediction of Superconductors Based on Machine Learning: A Short Review) [...] #### Exploiting Adapters for Cross-Lingual Low-Resource Speech Recognition)\\n\\n#### Hierarchical knowledge amalgamation with dual discriminative feature alignment)\\n\\n#### Modeling Dynamic Missingness of Implicit Feedback for Sequential Recommendation)\\n\\n#### S2SNet: A Pretrained Neural Network for Superconductivity Discovery)\\n\\n#### Learning Interest-oriented Universal User Representation via Self-supervision) [...] #### Learning Universal User Representations via Self-Supervised Lifelong Behaviors Modeling)\\n\\n#### Learning Invariant Representations across Domains and Tasks)\\n\\n#### Co-Authors\\n\\nOpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors. \u00a9 2025 OpenReview', 'score': 0.64047897, 'raw_content': None}, {'url': 'https://www.aminer.cn/profile/renjun-xu/53f42ceddabfaedd74d30355?source=bz1', 'title': 'Renjun Xu - Center for Data Science, Zhejiang University | \u4eba\u624d\u753b\u50cf', 'content': 'Renjun Xu - Center for Data Science, Zhejiang University | \u4eba\u624d\u753b\u50cf - AMiner\\n\\n\\n\\nResearch\\n\\nCenter for Data Science Zhejiang University\\n\\n\u3001\u300aInternational Joint Conference on Artificial Intelligence\u300b(IJCAI, CCF-A), \u300aIEEE Transactions on Knowledge and Data Engineering\u300b(TKDE, CCF-A)\u4ea4\u53c9\u9886\u57df\u53d1\u8868\u591a\u7bc7\u56fd\u9645\u9876\u5c16\u671f\u520a\u548c\u4f1a\u8bae\u6587\u7ae0\uff0cCVPR\u3001AAAI\u3001NIPS\u3001TPAMI\u3001TIP\u3001TLT\u7b49\u9876\u7ea7\u4eba\u5de5\u667a\u80fd\u671f\u520a\u548c\u4f1a\u8bae\u7a0b\u5e8f\u59d4\u5458\u4f1a\u59d4\u5458\uff0c\u8363\u83b72020\u5e74\u5ea6\u4e16\u754c\u4eba\u5de5\u667a\u80fd\u5927\u4f1a\u9752\u5e74\u4f18\u79c0\u8bba\u6587\u63d0\u540d\u5956\uff0c\u6307\u5bfc\u5e76\u63a8\u8350\u7684\u6240\u6709\u5b66\u751f\u5747\u5df2\u62ff\u5230\u9ebb\u7701\u7406\u5de5\u5b66\u9662(MIT)\u3001\u5361\u5185\u57fa\u6885\u9686\u5927\u5b66(CMU)\u7b49\u5168\u7403\u9876\u5c16\u540d\u6821\u7684offer\uff01\\n\\nEducation\\n\\nSign in to view more\\n\\nExperience\\n\\nSign in to view more [...] Research Interests\\n\\n2012 2025\\n\\nPapers \u5171 39 \u7bc7 Patents \u5171 9 \u7bc7 Author Statistics Co-Author Similar Experts\\n\\nBy Year By Citation \u4e3b\u9898\u7b5b\u9009 \u671f\u520a\u7ea7\u522b\u7b5b\u9009 \u5408\u4f5c\u8005\u7b5b\u9009 \u5408\u4f5c\u673a\u6784\u7b5b\u9009\\n\\n\u65f6\u95f4\\n\\n\u5f15\u7528\u91cf\\n\\n\u4e3b\u9898\\n\\n\u671f\u520a\u7ea7\u522b\\n\\n\u5408\u4f5c\u8005\\n\\n\u5408\u4f5c\u673a\u6784\\n\\nAll 2025 2024 2023 2022 2021 2020 2015 2014 2013 2012 2010 2006\\n\\nDo PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning Vs. Memorization in Large Language Models\\n\\nYang Yan,Yu Lu,Renjun Xu,Zhenzhong Lan\\n\\narXiv \u00b7 Computation and Language\uff082025\uff09\\n\\nCited 0 Views 11 Bibtex\\n\\n0\\n\\n11 [...] The page data are from open Internet sources, cooperative publishers and automatic analysis results through AI technology. We do not make any commitments and guarantees for the validity, accuracy, correctness, reliability, completeness and timeliness of the page data. If you have any questions, please contact us by email: report@aminer.cn\\n\\nSwipe to Fine Result', 'score': 0.60049355, 'raw_content': None}, {'url': 'https://hurenjun.github.io/', 'title': \"Renjun Hu's Homepage\", 'content': \"an algorithm engineer at Alibaba Cloud, contributing to AI-driven transformations across various business\\ndomains including feed recommendation, user growth, online marketing, and LLM-as-a-Judge.\\nSince January 2025, he has joined the School of Data Science and Engineering, East China Normal University\\nas a young researcher.\\nHis recent research interests include robust machine learning and the understanding, evaluation, and applications of large language models. [...] Renjun Hu received his Bachelor's degree in 2014 and Ph.D. in 2020 from the School of Computer Science\\nand Engineering at Beihang University. From September 2017 to April 2018, he was a joint Ph.D. student\\nin the Data Mining Group at Rutgers University. He then worked as a research intern at the Business\\nIntelligence Lab of Baidu Research from May 2018 to September 2019. During 2020 to 2024, he served as [...] Renjun's avatar\\n\\n### Renjun Hu     (\u80e1\u4ec1\u541b)\\n\\n#### Young Researcher\\n\\nSchool of Data Science of Engineering (DaSE)  \\nEast China Normal University (ECNU)\\n\\n \\nRoom X109, Shuxueguan, Putuo Campus\\n\\n \\nrjhu [at] dase.ecnu.edu.cn    renjun0hu [at] gmail.com\\n\\n### Short Bio\", 'score': 0.6002124, 'raw_content': None}, {'url': 'https://www.justice.gov/archives/opa/pr/chinese-government-intelligence-officer-sentenced-20-years-prison-espionage-crimes-attempting', 'title': 'Chinese Government Intelligence Officer Sentenced to 20 ...', 'content': 'On Nov. 5, 2021, a federal jury in Cincinnati convicted Xu on all counts: conspiracy to commit economic espionage, conspiracy to commit trade secret theft, attempted economic espionage and attempted trade secret theft.\\n\\nXu was a career intelligence officer, beginning in 2003 and rising to the rank of deputy division director at the Chinese Ministry of State Security (MSS), the intelligence and security agency for China. [...] \u201cThis case sends a clear message: we will hold accountable anyone attempting to steal American trade secrets,\u201d said U.S. Attorney Kenneth L. Parker for the Southern District of Ohio. \u201cXu conspired to steal American science and technology. Thanks to the diligent work of the FBI, GE Aviation and our trial team, he\u2019ll spend decades in federal prison.\u201d [...] According to court documents and trial testimony, beginning in at least December 2013, Xu targeted specific companies in the United States and abroad that are recognized as leaders in the field of aviation.', 'score': 0.43770394, 'raw_content': None}, {'url': 'https://en.wikipedia.org/wiki/Xu_Genjun', 'title': 'Xu Genjun - Wikipedia', 'content': \"Xu was born in She County, Anhui on 23 November 1935. He graduated from the Chemistry Department of Fudan University in 1957, and joined the Shanghai Institute of Biochemistry afterwards, where he participated in and made significant contributions to the insulin synthesis project led by Wang Yinglai. He later researched enzyme kinetics, chemical modification of proteins, the relationship between proteins' structure and function, and the folding and refolding of proteins. [...] Xu published more than 100 research papers in academic journals. He was a two-time winner of the State Natural Science Award (first class), in addition to several national prizes by the Chinese Academy of Sciences (CAS). He was also awarded the Ho Leung Ho Lee Prize for Life Sciences. He was elected as an academician of the CAS in 1991.\\n\\nXu died on 8 January 2008 at Zhongshan Hospital in Shanghai, at the age of 72.\\n\\n## References\\n\\nWikimedia Foundation\\nPowered by MediaWiki [...] Wikipedia\\nThe Free Encyclopedia\\n\\n## Contents\\n\\n# Xu Genjun\\n\\nXu Genjun (Chinese: \u8bb8\u6839\u4fca; 23 November 1935 \u2013 8 January 2008) was a Chinese biochemist. He was a professor at the Shanghai Institute of Biochemistry and Cell Biology. He was an academician of the Chinese Academy of Sciences and President of the Chinese Society of Biochemistry and Molecular Biology.\\n\\n## Biography\", 'score': 0.25395855, 'raw_content': None}], 'response_time': 2.74, 'request_id': 'a5a79643-d2c2-43e0-8511-ba48c477c34d'}), ToolMessage(content='[{\"title\": \"Jingwen Peng, CFA - Director - Lead Data Steward at Liberty Mutual ...\", \"url\": \"https://www.linkedin.com/in/jingwen-peng-cfa-3a69b011?trk=public_profile_browsemap\", \"content\": \"Director - Lead Data Steward at Liberty Mutual Investments \u00b7 Experience: Liberty Mutual Investments \u00b7 Location: Boston \u00b7 500+ connections on LinkedIn.\", \"score\": 0.70823324}, {\"title\": \"Jingwen Peng Email & Phone Number | Manulife Lead Analyst ...\", \"url\": \"https://rocketreach.co/jingwen-peng-email_28909123\", \"content\": \"Jingwen Peng holds a 2008 - 2009 Master of Arts in Mathematical Finance @ Boston University Questrom School of Business. With a robust skill set that includes\", \"score\": 0.6721816}, {\"title\": \"Jingwen Peng - Loop\", \"url\": \"https://loop.frontiersin.org/people/1381593/overview\", \"content\": \"Department of Medical Mycology, Institute of Dermatology, Chinese Academy of Medical Sciences. Nanjing, China. View All. mini profile avatar Jingwen Peng.\", \"score\": 0.6445166}, {\"title\": \"Jingwen Peng - U of Rochester Simon STEM MSBA - LinkedIn\", \"url\": \"https://www.linkedin.com/in/jpeng19\", \"content\": \"Implemented a CNN-based model (VGG-16) for classifying the attractiveness of celebrity faces with TensorFlow, attaining an accuracy rate exceeding 80%. Airline\", \"score\": 0.5617203}, {\"title\": \"Leaders of Tomorrow - Schwarzman Scholars\", \"url\": \"https://www.schwarzmanscholars.org/scholars/\", \"content\": \"Class of 2021 - 2022\\\\n\\\\nUnited States of America - University of Pennsylvania\\\\n\\\\n### Jing Sun\\\\n\\\\nClass of 2023 - 2024\\\\n\\\\nChina - King\\'s College London\\\\n\\\\n### Jingwen Sun\\\\n\\\\nClass of 2017 - 2018\\\\n\\\\nChina - National University of Singapore\\\\n\\\\n### Ruochen Sun\\\\n\\\\nClass of 2024 - 2025\\\\n\\\\nChina - Tulane University, Brown University\\\\n\\\\nView Bio\\\\n\\\\n### Alexander Sundberg\\\\n\\\\nClass of 2024 - 2025\\\\n\\\\nUnited States of America - Yale University\\\\n\\\\nView Bio\\\\n\\\\n### Sara Surani\\\\n\\\\nClass of 2020 - 2021 [...] United States of America - Massachusetts Institute of Technology\\\\n\\\\n### Fei PENG\\\\n\\\\nClass of 2018 - 2019\\\\n\\\\nChina - New York University, Pennsylvania State University\\\\n\\\\n### Keyang PENG\\\\n\\\\nClass of 2018 - 2019\\\\n\\\\nChina - Beijing Normal University\\\\n\\\\n### Yucheng Peng\\\\n\\\\nClass of 2021 - 2022\\\\n\\\\nChina - University of Chicago\\\\n\\\\n### Nicholas Peoples\\\\n\\\\nClass of 2023 - 2024\\\\n\\\\nUnited States of America - Duke University\\\\n\\\\n### William Peracchio\\\\n\\\\nClass of 2021 - 2022\\\\n\\\\nUnited States of America - Lehigh University [...] China - University of California, Berkeley\\\\n\\\\n### Gabrijela Papec\\\\n\\\\nClass of 2025 - 2026\\\\n\\\\nCroatia - Sciences Po\\\\n\\\\nView Bio\\\\n\\\\n### Cristina Parajon\\\\n\\\\nClass of 2018 - 2019\\\\n\\\\nNicaragua - Harvard University\\\\n\\\\n### Jinwan Park\\\\n\\\\nClass of 2024 - 2025\\\\n\\\\nSouth Korea - University of Wisconsin-Madison\\\\n\\\\nView Bio\\\\n\\\\n### Collin Parker\\\\n\\\\nClass of 2017 - 2018\\\\n\\\\nUnited States of America - United States Military Academy at West Point\\\\n\\\\n### Regina Parker\\\\n\\\\nClass of 2016 - 2017\", \"score\": 0.20904993}]', name='tavily_search_results_json', id='ef3627c7-202c-4db1-8aaf-52734fa663a3', tool_call_id='call_onKlxlSCDfP3tsjIWu4q1do5', artifact={'query': 'Jingwen Peng', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.linkedin.com/in/jingwen-peng-cfa-3a69b011?trk=public_profile_browsemap', 'title': 'Jingwen Peng, CFA - Director - Lead Data Steward at Liberty Mutual ...', 'content': 'Director - Lead Data Steward at Liberty Mutual Investments \u00b7 Experience: Liberty Mutual Investments \u00b7 Location: Boston \u00b7 500+ connections on LinkedIn.', 'score': 0.70823324, 'raw_content': None}, {'url': 'https://rocketreach.co/jingwen-peng-email_28909123', 'title': 'Jingwen Peng Email & Phone Number | Manulife Lead Analyst ...', 'content': 'Jingwen Peng holds a 2008 - 2009 Master of Arts in Mathematical Finance @ Boston University Questrom School of Business. With a robust skill set that includes', 'score': 0.6721816, 'raw_content': None}, {'url': 'https://loop.frontiersin.org/people/1381593/overview', 'title': 'Jingwen Peng - Loop', 'content': 'Department of Medical Mycology, Institute of Dermatology, Chinese Academy of Medical Sciences. Nanjing, China. View All. mini profile avatar Jingwen Peng.', 'score': 0.6445166, 'raw_content': None}, {'url': 'https://www.linkedin.com/in/jpeng19', 'title': 'Jingwen Peng - U of Rochester Simon STEM MSBA - LinkedIn', 'content': 'Implemented a CNN-based model (VGG-16) for classifying the attractiveness of celebrity faces with TensorFlow, attaining an accuracy rate exceeding 80%. Airline', 'score': 0.5617203, 'raw_content': None}, {'url': 'https://www.schwarzmanscholars.org/scholars/', 'title': 'Leaders of Tomorrow - Schwarzman Scholars', 'content': \"Class of 2021 - 2022\\n\\nUnited States of America - University of Pennsylvania\\n\\n### Jing Sun\\n\\nClass of 2023 - 2024\\n\\nChina - King's College London\\n\\n### Jingwen Sun\\n\\nClass of 2017 - 2018\\n\\nChina - National University of Singapore\\n\\n### Ruochen Sun\\n\\nClass of 2024 - 2025\\n\\nChina - Tulane University, Brown University\\n\\nView Bio\\n\\n### Alexander Sundberg\\n\\nClass of 2024 - 2025\\n\\nUnited States of America - Yale University\\n\\nView Bio\\n\\n### Sara Surani\\n\\nClass of 2020 - 2021 [...] United States of America - Massachusetts Institute of Technology\\n\\n### Fei PENG\\n\\nClass of 2018 - 2019\\n\\nChina - New York University, Pennsylvania State University\\n\\n### Keyang PENG\\n\\nClass of 2018 - 2019\\n\\nChina - Beijing Normal University\\n\\n### Yucheng Peng\\n\\nClass of 2021 - 2022\\n\\nChina - University of Chicago\\n\\n### Nicholas Peoples\\n\\nClass of 2023 - 2024\\n\\nUnited States of America - Duke University\\n\\n### William Peracchio\\n\\nClass of 2021 - 2022\\n\\nUnited States of America - Lehigh University [...] China - University of California, Berkeley\\n\\n### Gabrijela Papec\\n\\nClass of 2025 - 2026\\n\\nCroatia - Sciences Po\\n\\nView Bio\\n\\n### Cristina Parajon\\n\\nClass of 2018 - 2019\\n\\nNicaragua - Harvard University\\n\\n### Jinwan Park\\n\\nClass of 2024 - 2025\\n\\nSouth Korea - University of Wisconsin-Madison\\n\\nView Bio\\n\\n### Collin Parker\\n\\nClass of 2017 - 2018\\n\\nUnited States of America - United States Military Academy at West Point\\n\\n### Regina Parker\\n\\nClass of 2016 - 2017\", 'score': 0.20904993, 'raw_content': None}], 'response_time': 3.77, 'request_id': '35ed8a30-b940-4944-8a77-3a692c7b921f'})]\n",
      "\n",
      "\n",
      "\n",
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='Renjun Xu is currently a Principal Researcher at Zhejiang University. Jingwen Peng is a Director and Lead Data Steward at Liberty Mutual Investments in Boston.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 6044, 'total_tokens': 6075, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2816}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CJ4iPtlPcv3ePTQdgHsB7GvW9Cibv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b7a683c9-4576-42b2-be2b-d8b8d3a0b526-0', usage_metadata={'input_tokens': 6044, 'output_tokens': 31, 'total_tokens': 6075, 'input_token_details': {'audio': 0, 'cache_read': 2816}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the A Comprehensive Survey of Deep Research paper, then search each of the authors to find out where they work now using Tavily!\")]}\n",
    "\n",
    "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        if node == \"action\":\n",
    "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
    "        print(values[\"messages\"])\n",
    "\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXzDlZVz1Hnf"
   },
   "source": [
    "#### \ud83c\udfd7\ufe0f Activity #2:\n",
    "\n",
    "Please write out the steps the agent took to arrive at the correct answer.\n",
    "\n",
    "ANSWER\u2705\n",
    "\n",
    "1.) Request Input \u2013 The user\u2019s request was defined and the state object was populated. In this example, the request was to search for a specific research paper, extract the authors\u2019 names, and then find out where they currently work.\n",
    "\n",
    "2.) Agent Node Decision \u2013 The state object was passed into the agent node (entry point). The agent node added an AIMessage to the state object and determined which tools to use. Here, Arxiv was used to retrieve the paper and author names, and Tavily was used to look up the authors\u2019 current workplaces.\n",
    "\n",
    "3.) Action Nodes Execute Tools \u2013 Each action node executed its respective tool. The responses from Arxiv and Tavily were added to the state object and sent back along the conditional edge to the agent node.\n",
    "\n",
    "4.)Agent Node Updates \u2013 The agent node processed the tool responses, added a summary of the authors\u2019 workplaces to the state object, and passed it along the conditional edge.\n",
    "\n",
    "5.) Conditional Edge & END \u2013 The conditional edge received the state object, found no further \u201ctool_calls\u201d required, and passed the state object to END.\n",
    "\n",
    "6.) Output \u2013 The final state object contains the original request, all tool responses, and the summarized author information. Each chunk in the output reflects updates from a specific node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83e\udd1d Breakout Room #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7c8-Uyarh1v"
   },
   "source": [
    "## Part 1: LangSmith Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV3XeFOT1Sar"
   },
   "source": [
    "### Pre-processing for LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wruQCuzewUuO"
   },
   "source": [
    "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "orYxBZXSxJjZ",
    "outputId": "76be837b-6424-4516-8f63-07fbd8c25bf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Deep Research typically refers to an in-depth and comprehensive investigation or analysis into a specific topic, subject, or field. It involves gathering detailed information, examining various sources, and analyzing data thoroughly to gain a profound understanding of the subject matter. Deep Research is often used in academic, scientific, technological, and business contexts to develop insights, inform decision-making, or advance knowledge.\\n\\nIf you are referring to a specific organization, product, or service named \"Deep Research,\" please provide more context so I can give a more precise answer.'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_inputs(input_object):\n",
    "  return {\"messages\" : [HumanMessage(content=input_object[\"text\"])]}\n",
    "\n",
    "def parse_output(input_state):\n",
    "  return {\"answer\" : input_state[\"messages\"][-1].content}\n",
    "\n",
    "agent_chain_with_formatting = convert_inputs | simple_agent_graph | parse_output\n",
    "\n",
    "agent_chain_with_formatting.invoke({\"text\" : \"What is Deep Research?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9UkCIqkpyZu"
   },
   "source": [
    "### Task 1: Creating An Evaluation Dataset\n",
    "\n",
    "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
    "\n",
    "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
    "\n",
    "```python\n",
    "questions = [\n",
    "    {\n",
    "        \"inputs\" : {\"text\" : \"Who were the main authors on the 'A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications' paper?\"},\n",
    "        \"outputs\" : {\"must_mention\" : [\"Peng\", \"Xu\"]}   \n",
    "    },\n",
    "    ...,\n",
    "    {\n",
    "        \"inputs\" : {\"text\" : \"Where do the authors of the 'A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications' work now?\"},\n",
    "        \"outputs\" : {\"must_mention\" : [\"Zhejiang\", \"Liberty Mutual\"]}\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfMXF2KAsQxs"
   },
   "source": [
    "#### \ud83c\udfd7\ufe0f Activity #3:\n",
    "\n",
    "Please create a dataset in the above format with at least 5 questions that pertain to the cohort use-case (more information [here](https://www.notion.so/Session-4-RAG-with-LangGraph-OSS-Local-Models-Eval-w-LangSmith-26acd547af3d80838d5beba464d7e701#26acd547af3d81d08809c9c82a462bdd)), or the use-case you're hoping to tackle in your Demo Day project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "CbagRuJop83E"
   },
   "outputs": [],
   "source": [
    "\n",
    "##My demo day project idea as of now: Demo Day Project \u2013 Event Evaluator: A multi-agent application that analyzes Northern VA traffic conditions by pulling data from news sources and real-time incident APIs. \n",
    "##It identifies the causes of congestion, summarizes incidents, and generates comprehensive reports or speeches, helping city planners, commuters, and organizations understand why traffic occurred and communicate insights effectively.\n",
    "\n",
    "\n",
    "\n",
    "questions = [\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"What caused the traffic congestion on I-95 General Purpose lanes in GWA region yesterday morning?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"accident\", \"construction\", \"weather\", \"incident\", \"special event\"]}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Which areas in Northern Virginia had the longest commute times yesterday and why?\"},\n",
    "        \"outputs\": {\"must_mention\": [\"Arlington\", \"Fairfax\", \"Alexandria\", \"Tysons\", \"delays\"]}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Summarize yesterday's major traffic incidents for a local news bulletin.\"},\n",
    "        \"outputs\": {\"must_mention\": [\"accidents\", \"road closures\", \"highways\", \"emergency response\"]}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Write a brief speech for city council explaining why traffic was unusually heavy yesterday.\"},\n",
    "        \"outputs\": {\"must_mention\": [\"public safety\", \"traffic management\", \"incident highlights\"]}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Create a commuter alert explaining alternative routes to avoid yesterday's traffic delays.\"},\n",
    "        \"outputs\": {\"must_mention\": [\"alternative routes\", \"delays\", \"highways\", \"traffic updates\"]}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"text\": \"Analyze yesterday's traffic events to suggest improvements for emergency response.\"},\n",
    "        \"outputs\": {\"must_mention\": [\"response times\", \"incident handling\", \"recommendations\", \"traffic flow\"]}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7QVFuAmsh7L"
   },
   "source": [
    "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "RLfrZrgSsn85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['53fea113-6752-498f-a55c-30588cde8fc9',\n",
       "  'a008d2ab-de04-4385-ad8c-ee7344a4ac34',\n",
       "  '207b98dc-f3bb-4e00-9626-e0f0bbba4fc3',\n",
       "  '2aeb386b-849e-4799-8967-7159c006d640',\n",
       "  '4dabbc28-f52c-4ace-a1bf-2e9b63791fb8',\n",
       "  '63f65d04-ecc8-4a9f-bcbb-8fa6e5501361'],\n",
       " 'count': 6}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = f\"Simple Search Agent - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Questions about potential demo day idea to evaluate the Simple Search Agent.\"\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    examples=questions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lRTXUrTtP9Y"
   },
   "source": [
    "### Task 2: Adding Evaluators\n",
    "\n",
    "Let's use the OpenEvals library to product an evaluator that we can then pass into LangSmith!\n",
    "\n",
    "> NOTE: Examine the `CORRECTNESS_PROMPT` below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert data labeler evaluating model outputs for correctness. Your task is to assign a score based on the following rubric:\n",
      "\n",
      "<Rubric>\n",
      "  A correct answer:\n",
      "  - Provides accurate and complete information\n",
      "  - Contains no factual errors\n",
      "  - Addresses all parts of the question\n",
      "  - Is logically consistent\n",
      "  - Uses precise and accurate terminology\n",
      "\n",
      "  When scoring, you should penalize:\n",
      "  - Factual errors or inaccuracies\n",
      "  - Incomplete or partial answers\n",
      "  - Misleading or ambiguous statements\n",
      "  - Incorrect terminology\n",
      "  - Logical inconsistencies\n",
      "  - Missing key information\n",
      "</Rubric>\n",
      "\n",
      "<Instructions>\n",
      "  - Carefully read the input and output\n",
      "  - Check for factual accuracy and completeness\n",
      "  - Focus on correctness of information rather than style or verbosity\n",
      "</Instructions>\n",
      "\n",
      "<Reminder>\n",
      "  The goal is to evaluate factual correctness and completeness of the response.\n",
      "</Reminder>\n",
      "\n",
      "<input>\n",
      "{inputs}\n",
      "</input>\n",
      "\n",
      "<output>\n",
      "{outputs}\n",
      "</output>\n",
      "\n",
      "Use the reference outputs below to help you evaluate the correctness of the response:\n",
      "\n",
      "<reference_outputs>\n",
      "{reference_outputs}\n",
      "</reference_outputs>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openevals.prompts import CORRECTNESS_PROMPT\n",
    "print(CORRECTNESS_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "QrAUXMFftlAY"
   },
   "outputs": [],
   "source": [
    "from openevals.llm import create_llm_as_judge\n",
    "\n",
    "correctness_evaluator = create_llm_as_judge(\n",
    "        prompt=CORRECTNESS_PROMPT,\n",
    "        model=\"openai:o3-mini\", # very impactful to the final score\n",
    "        feedback_key=\"correctness\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a custom Evaluator for our created dataset above - we do this by first making a simple Python function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def must_mention(inputs: dict, outputs: dict, reference_outputs: dict) -> float:\n",
    "  # determine if the phrases in the reference_outputs are in the outputs\n",
    "  required = reference_outputs.get(\"must_mention\") or []\n",
    "  score = all(phrase in outputs[\"answer\"] for phrase in required)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNtHORUh0jZY"
   },
   "source": [
    "#### \u2753 Question #4:\n",
    "\n",
    "What are some ways you could improve this metric as-is?\n",
    "\n",
    "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
    "\n",
    "ANSWER \u2705\n",
    "\n",
    "The initial evaluation metric is a good starting point but primarily measures keyword recall and factual reporting. It could be improved by diversifying question types to include analytical and predictive tasks, adding granularity to expected outputs (e.g., numeric values or time ranges), and introducing ambiguity to test reasoning under uncertainty. While the current evaluation relies heavily on keywords, I should broaden the keyword set and add tests that better assess reasoning ability and output complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1RJr349zhv7"
   },
   "source": [
    "Task 3: Evaluating\n",
    "\n",
    "All that is left to do is evaluate our agent's response!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "efcf57067cf743d8b4ce059a61cbe02e",
      "53e33aae3b97490c82aec7bbb0d6ebba",
      "ad84e0e971d3455db2efe7dd0d1f803e",
      "72adef9b70dd48198b7322b6c5b113cf",
      "8a61d045ffd44ac58f3f13eb10044836",
      "041e22a9b5514e36bd4d1dac01d5d398",
      "886d762f2a7c421382efb5502c6d42a1",
      "ab91fd625bbd43afbf8c6398193a88d0",
      "716557ad09874dcb989d75f7c74424cd",
      "77d4c0ebaae045b58efc4f789c9a2360",
      "0d622ccc56264fac8fd7508dbdbe6e29"
     ]
    },
    "id": "p5TeCUUkuGld",
    "outputId": "2f7d62a2-e78d-447a-d07b-f9e4d500fb79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'simple_agent, baseline-2a219cdf' at:\n",
      "https://smith.langchain.com/o/bf3a953b-d5ae-479e-b06a-74d94431c40b/datasets/b5a20e55-36b9-40b5-82f0-d12b044e05f9/compare?selectedSessions=94858c9c-fd31-45c8-907b-0b541a32bb33\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccda2da12dca458fb35dbc1b07cf5b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = client.evaluate(\n",
    "    agent_chain_with_formatting,\n",
    "    data=dataset.name,\n",
    "    evaluators=[correctness_evaluator, must_mention],\n",
    "    experiment_prefix=\"simple_agent, baseline\",  # optional, experiment name prefix\n",
    "    description=\"Testing the baseline system.\",  # optional, experiment description\n",
    "    max_concurrency=4, # optional, add concurrency\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhTNe4kWrplB"
   },
   "source": [
    "## Part 2: LangGraph with Helpfulness:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1wKRddbIY_S"
   },
   "source": [
    "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
    "\n",
    "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
    "\n",
    "We're going to make a few key adjustments to account for this:\n",
    "\n",
    "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
    "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npTYJ8ayR5B3"
   },
   "source": [
    "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "-LQ84YhyJG0w"
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD7EV0HqSQcb"
   },
   "source": [
    "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oajBwLkFVi1N"
   },
   "source": [
    "#### \ud83c\udfd7\ufe0f Activity #4:\n",
    "\n",
    "Please write markdown for the following cells to explain what each is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6rN7feNVn9f"
   },
   "source": [
    "## Creating an Enhanced State Graph\n\nThis cell creates a new  instance called  that will include an additional helpfulness evaluation step. This graph will be similar to our previous agent but with an added quality control mechanism to ensure the agent's responses are truly helpful to the user.\n\nThe graph starts with the same two core nodes:\n- : The main reasoning node that calls the language model\n- : The tool execution node that runs any required tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6r6XXA5FJbVf",
    "outputId": "ff713041-e498-4f0f-a875-a03502b87729"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x75bc729039d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_helpfulness_check = StateGraph(AgentState)\n",
    "\n",
    "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
    "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ22o2mWVrfp"
   },
   "source": [
    "## Setting the Entry Point\n\nThis cell establishes that our enhanced agent will start with the  node, just like our previous simple agent. The agent will begin by processing the user's input and deciding whether to use tools or provide a direct response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNWHwWxuRiLY",
    "outputId": "295f5a35-ceff-452a-ffb8-c52eada6a816"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x75bc729039d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_helpfulness_check.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsXeF6xlaXOZ"
   },
   "source": [
    "## Implementing the Helpfulness Check Logic\n\nThis cell defines the  function, which is the key enhancement to our agent. This function serves as a conditional router that:\n\n1. **Checks for tool calls**: If the agent wants to use tools, it routes to the \"action\" node\n2. **Implements a helpfulness evaluation**: If no tools are needed, it evaluates whether the response is truly helpful\n3. **Prevents infinite loops**: Stops execution after 10 message exchanges to avoid endless conversations\n4. **Uses a separate AI model**: Employs GPT-4.1-mini to objectively evaluate response quality\n\nThe helpfulness check compares the original user query with the agent's final response and asks a specialized model to determine if the response adequately addresses the user's needs (Y for helpful, N for unhelpful).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "z_Sq3A9SaV1O"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def tool_call_or_helpful(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  if last_message.tool_calls:\n",
    "    return \"action\"\n",
    "\n",
    "  initial_query = state[\"messages\"][0]\n",
    "  final_response = state[\"messages\"][-1]\n",
    "\n",
    "  if len(state[\"messages\"]) > 10:\n",
    "    return \"END\"\n",
    "\n",
    "  prompt_template = \"\"\"\\\n",
    "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
    "\n",
    "  Initial Query:\n",
    "  {initial_query}\n",
    "\n",
    "  Final Response:\n",
    "  {final_response}\"\"\"\n",
    "\n",
    "  helpfullness_prompt_template = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "\n",
    "  helpfulness_chain = helpfullness_prompt_template | helpfulness_check_model | StrOutputParser()\n",
    "\n",
    "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
    "\n",
    "  if \"Y\" in helpfulness_response:\n",
    "    return \"end\"\n",
    "  else:\n",
    "    return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BhnBW2YVsJO"
   },
   "source": [
    "## Configuring the Decision Logic\n\nThis cell adds the conditional routing logic to our graph. The  method connects the \"agent\" node to different paths based on the  function's output:\n\n- **\"continue\" \u2192 \"agent\"**: If the response isn't helpful enough, the agent tries again\n- **\"action\" \u2192 \"action\"**: If tools are needed, execute them\n- **\"end\" \u2192 END**: If the response is helpful, conclude the conversation\n\nThis creates a feedback loop that allows the agent to refine its responses until they meet quality standards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVTKnWMbP_8T",
    "outputId": "7f729b1f-311c-4084-ceaf-0da437900c85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x75bc729039d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_helpfulness_check.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tool_call_or_helpful,\n",
    "    {\n",
    "        \"continue\" : \"agent\",\n",
    "        \"action\" : \"action\",\n",
    "        \"end\" : END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGDLEWOIVtK0"
   },
   "source": [
    "## Connecting Action Back to Agent\n\nThis cell ensures that after executing tools in the \"action\" node, the agent returns to the \"agent\" node to process the tool results and potentially generate a final response. This maintains the flow where tool execution is followed by reasoning about the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbDK2MbuREgU",
    "outputId": "21a64c20-27a1-4e0e-afde-a639abaa8b55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x75bc729039d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSI8AOaEVvT-"
   },
   "source": [
    "## Compiling the Enhanced Agent\n\nThis cell compiles our enhanced state graph into an executable agent called . The compilation process optimizes the graph structure and prepares it for execution with all the conditional logic and helpfulness evaluation in place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "oQldl8ERQ8lf"
   },
   "outputs": [],
   "source": [
    "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F67FGCMRVwGz"
   },
   "source": [
    "## Testing the Helpfulness-Enhanced Agent\n\nThis cell demonstrates the enhanced agent in action by asking \"What are Deep Research Agents?\" The agent:\n\n1. **Processes the query** through the main agent node\n2. **Evaluates its own response** using the helpfulness check\n3. **Decides whether to continue or end** based on response quality\n4. **Streams the results** showing the agent's reasoning process\n\nThe output shows that the agent provided a comprehensive response about Deep Research Agents and the helpfulness check determined it was satisfactory, so the conversation ended appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3oo8E-PRK1T",
    "outputId": "f152dea8-96ad-4d29-d8b2-a064c96a8bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving update from node: 'agent'\n",
      "[AIMessage(content='Deep Research Agents are advanced AI systems designed to assist with in-depth research tasks. They leverage deep learning techniques and large datasets to analyze complex information, generate insights, and support decision-making across various fields such as science, technology, medicine, and more. These agents can automate literature reviews, extract relevant data from vast sources, and provide comprehensive summaries, making research more efficient and thorough. Would you like me to find more detailed or specific information about Deep Research Agents?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 158, 'total_tokens': 251, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CJ4p3pIKceAjbeS3g3QtsJLgucObk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b10b2082-4074-4d48-848f-37d11d18949b-0', usage_metadata={'input_tokens': 158, 'output_tokens': 93, 'total_tokens': 251, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\" : [HumanMessage(content=\"What are Deep Research Agents?\")]}\n",
    "\n",
    "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
    "    for node, values in chunk.items():\n",
    "        print(f\"Receiving update from node: '{node}'\")\n",
    "        print(values[\"messages\"])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVmZPs6lnpsM"
   },
   "source": [
    "## Part 3: LangGraph for the \"Patterns\" of GenAI\n",
    "\n",
    "### Task 4: Helpfulness Check of Gen AI Pattern Descriptions\n",
    "\n",
    "Let's ask our system about the 3 main patterns in Generative AI:\n",
    "\n",
    "1. Context Engineering\n",
    "2. Fine-tuning\n",
    "3. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZoLl7GlXoae-"
   },
   "outputs": [],
   "source": [
    "patterns = [\"Context Engineering\", \"Fine-tuning\", \"LLM-based agents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zkh0YJuCp3Zl",
    "outputId": "d847426e-71b3-47e6-b1ae-351a78d68d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Engineering is a relatively new interdisciplinary field that focuses on designing, managing, and optimizing the context in which systems, especially artificial intelligence and software applications, operate. It involves understanding and shaping the environment, circumstances, and background information that influence how systems behave and interact with users. The goal is to improve system performance, user experience, and decision-making by carefully engineering the context.\n",
      "\n",
      "The concept of Context Engineering has gained prominence with the rise of AI, IoT, and complex software systems, where context plays a crucial role in system effectiveness. It started to break onto the scene in the late 2010s and early 2020s, driven by advancements in contextual AI, ubiquitous computing, and the need for more adaptive and personalized systems.\n",
      "\n",
      "Would you like me to find more detailed or specific information about its origins and development?\n",
      "\n",
      "\n",
      "\n",
      "Fine-tuning is a machine learning technique used to adapt a pre-trained model to a specific task or dataset. Instead of training a model from scratch, which can be resource-intensive and time-consuming, fine-tuning involves taking an existing model that has already learned general features from a large dataset and then further training it on a smaller, task-specific dataset. This process helps the model specialize and improve its performance on the target task while leveraging the knowledge it has already acquired.\n",
      "\n",
      "Fine-tuning has become a prominent approach in the development of large language models, computer vision models, and other deep learning architectures. It allows researchers and developers to efficiently customize models for various applications, such as natural language processing, image recognition, and more.\n",
      "\n",
      "As for when it \"broke onto the scene,\" fine-tuning has been around in various forms for many years, but it gained widespread popularity and recognition with the rise of large pre-trained models like BERT (introduced in 2018) and GPT (with GPT-2 in 2019 and GPT-3 in 2020). These models demonstrated the effectiveness of transfer learning and fine-tuning, leading to a significant shift in how AI models are developed and deployed.\n",
      "\n",
      "Would you like me to find more detailed historical information or specific milestones related to the development of fine-tuning?\n",
      "\n",
      "\n",
      "\n",
      "LLM-based agents are intelligent systems that leverage large language models (LLMs) to perform a variety of tasks, such as understanding natural language, generating human-like responses, and making decisions or taking actions based on the input they receive. These agents can be used in applications like chatbots, virtual assistants, automated customer support, and more complex decision-making systems.\n",
      "\n",
      "The concept of LLM-based agents gained significant attention and broke onto the scene around 2020-2021, coinciding with the development and release of advanced large language models like OpenAI's GPT-3 in 2020. GPT-3's impressive capabilities demonstrated the potential of LLMs to serve as foundational components for building intelligent agents that can understand and generate human language at a high level.\n",
      "\n",
      "Since then, the field has rapidly evolved, with numerous innovations in model architecture, training techniques, and applications, making LLM-based agents a prominent area of research and development in artificial intelligence.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pattern in patterns:\n",
    "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
    "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
    "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
    "  print(messages[\"messages\"][-1].content)\n",
    "  print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "LangGraph Agent",
   "language": "python",
   "name": "langgraph-agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}